{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a00524cd016c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m \u001b[1;31m# linear algebra\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[1;31m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_importance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import xgboost\n",
    "import csv as csv\n",
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "from sklearn.model_selection  import  train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV   #Perforing grid search\n",
    "from scipy.stats import skew, shapiro\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=pd.read_csv('house/train.csv', header=0)\n",
    "test_dataset=pd.read_csv('house/test.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features=['MSSubClass','MSZoning','Street','Alley','LotShape','LandContour','Utilities',\n",
    "                      'LotConfig','LandSlope','Neighborhood','Condition1','Condition2','BldgType','HouseStyle',\n",
    "                      'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','ExterQual','ExterCond',\n",
    "                      'Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Heating',\n",
    "                      'HeatingQC','CentralAir','Electrical','KitchenQual','Functional','FireplaceQu',\n",
    "                     'GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive','PoolQC','Fence',\n",
    "                     'MiscFeature','SaleType','SaleCondition']\n",
    "every_column_except_y= [col for col in train_dataset.columns if col not in ['SalePrice','Id']]\n",
    "train_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.SalePrice = np.log1p(train_dataset.SalePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba de Shapiro-Wilk para normalidad\n",
    "\n",
    "# En caso de que tras aplicar el logaritmo sea más normal, entonces quedate con el logaritmo\n",
    "\n",
    "every_column_non_categorical= [col for col in train_dataset.columns if col not in categorical_features and col not in ['Id', 'SalePrice'] ]\n",
    "numeric_feats = train_dataset[every_column_non_categorical].dtypes[train_dataset.dtypes != \"object\"].index\n",
    "\n",
    "for feat in numeric_feats:\n",
    "    x = train_dataset[feat].dropna().values\n",
    "    if (shapiro(x)[1] > shapiro(np.log1p(x))[1]):\n",
    "        print(feat)\n",
    "        \n",
    "    else:\n",
    "        print(\"log\")\n",
    "        train_dataset[feat] = np.log1p(train_dataset[feat])\n",
    "        test_dataset[feat] = np.log1p(test_dataset[feat].values)\n",
    "\n",
    "print(shapiro(x))\n",
    "print(shapiro(np.log1p(x)))\n",
    "\n",
    "#shapiro(train_dataset.MasVnrArea.dropna().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset[train_dataset.GrLivArea < 8.25]\n",
    "train_dataset = train_dataset[train_dataset.LotArea < 11.5]\n",
    "train_dataset = train_dataset[train_dataset.SalePrice<13]\n",
    "train_dataset = train_dataset[train_dataset.SalePrice>10.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputar datos faltantes\n",
    "from fancyimpute import KNN, NuclearNormMinimization, SoftImpute, BiScaler\n",
    "\n",
    "# Definir método\n",
    "impt = SoftImpute()\n",
    "\n",
    "every_column_non_categorical= [col for col in train_dataset.columns if col not in categorical_features and col not in ['Id'] ]\n",
    "numeric_feats = train_dataset[every_column_non_categorical].dtypes[train_dataset.dtypes != \"object\"].index\n",
    "train_dataset[numeric_feats] = impt.fit_transform(train_dataset[numeric_feats])\n",
    "\n",
    "every_column_non_categorical= [col for col in test_dataset.columns if col not in categorical_features and col not in ['Id'] ]\n",
    "numeric_feats = test_dataset[every_column_non_categorical].dtypes[test_dataset.dtypes != \"object\"].index\n",
    "test_dataset[numeric_feats] = impt.fit_transform(test_dataset[numeric_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_dataset.OverallQual, train_dataset.SalePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(train_dataset.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fancyimpute import KNN, NuclearNormMinimization, SoftImpute, BiScaler\n",
    "#every_column_non_categorical= [col for col in train_dataset.columns if col not in categorical_features and col not in ['Id'] ]\n",
    "#numeric_feats = train_dataset[every_column_non_categorical].dtypes[train_dataset.dtypes != \"object\"].index\n",
    "#impt = SoftImpute()\n",
    "##impt = NuclearNormMinimization()\n",
    "#train_dataset[numeric_feats] = np.log1p(impt.fit_transform(train_dataset[numeric_feats]))\n",
    "\n",
    "#every_column_non_categorical= [col for col in test_dataset.columns if col not in categorical_features and col not in ['Id'] ]\n",
    "#numeric_feats = test_dataset[every_column_non_categorical].dtypes[test_dataset.dtypes != \"object\"].index\n",
    "#test_dataset[numeric_feats] = np.log1p(impt.fit_transform(test_dataset[numeric_feats]))\n",
    "#train_dataset[numeric_feats].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset.YearBuilt = train_dataset.YearBuilt.astype(\"str\")\n",
    "#test_dataset.YearBuilt = test_dataset.YearBuilt.astype(\"str\")\n",
    "\n",
    "# Calcular porcentage de faltantes por cada caracteristica categortica\n",
    "\n",
    "features_with_nan=['Alley','MasVnrType','BsmtQual','BsmtQual','BsmtCond','BsmtCond','BsmtExposure',\n",
    "                   'BsmtFinType1','BsmtFinType2','FireplaceQu','GarageType','GarageFinish']\n",
    "\n",
    "for feat in features_with_nan:\n",
    "    print(feat, train_dataset[feat].isna().sum()/len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[\"Alley\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_with_nan=['Alley','MasVnrType','BsmtQual','BsmtQual','BsmtCond','BsmtCond','BsmtExposure',\n",
    "                   'BsmtFinType1','BsmtFinType2','FireplaceQu','GarageType','GarageFinish']\n",
    "#function that creates a column for every value it might have\n",
    "#def ConverNaNToNAString(data, columnList):\n",
    "#    for x in columnList:       \n",
    "#        data[x] =str(data[x])              \n",
    "for feat in features_with_nan:\n",
    "    train_dataset[feat] = train_dataset[feat].fillna(\"NaN\") \n",
    "    test_dataset[feat] = test_dataset[feat].fillna(\"NaN\") \n",
    "\n",
    "#train_dataset[\"Alley\"] = train_dataset[\"Alley\"].fillna(\"NaN\")\n",
    "#ConverNaNToNAString(train_dataset, features_with_nan)\n",
    "#ConverNaNToNAString(test_dataset, features_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear columnas para cada categoría\n",
    "train_dataset = pd.get_dummies(train_dataset,columns =categorical_features.append(\"YearBuilt\"))\n",
    "test_dataset = pd.get_dummies(test_dataset,columns =categorical_features.append(\"YearBuilt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost.XGBRegressor(colsample_bytree=0.4,\n",
    "                 gamma=0,                 \n",
    "                 learning_rate=0.07,\n",
    "                 max_depth=3,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=10000,                                                                    \n",
    "                 reg_alpha=0.75,\n",
    "                 reg_lambda=0.45,\n",
    "                 subsample=0.6,\n",
    "                 seed=42, n_jobs=6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "every_column_except_y= [col for col in train_dataset.columns if col not in ['SalePrice','Id']]\n",
    "model.fit(train_dataset[every_column_except_y],train_dataset['SalePrice'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caracteristicas mas importantes\n",
    "OrderedDict(sorted(model.get_booster().get_score(importance_type='weight').items(), key=lambda t: t[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters = OrderedDict(sorted(model.get_booster().get_score(importance_type='weight').items(), key=lambda t: t[1], reverse=True))\n",
    "most_relevant_features= list( dict((k, v) for k, v in model.get_booster().get_score(importance_type='weight').items() if v >= 10).keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(train_dataset[every_column_except_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_log_error(train_dataset[\"SalePrice\"].values, model.predict(train_dataset[every_column_except_y])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(16,12))\n",
    "sns.heatmap(train_dataset[most_relevant_features].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "train_dataset.corr()[\"SalePrice\"].sort_values().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quitar Outliers\n",
    "train_dataset = train_dataset[train_dataset.GrLivArea < 8.25]\n",
    "train_dataset = train_dataset[train_dataset.LotArea < 11.5]\n",
    "train_dataset = train_dataset[train_dataset.SalePrice<13]\n",
    "train_dataset = train_dataset[train_dataset.SalePrice>10.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "colms = \"+\".join(numeric_feats)\n",
    "mdLineal = sm.OLS(train_dataset[\"SalePrice\"].values, train_dataset[most_relevant_features]).fit()\n",
    "np.sqrt(mean_squared_log_error(train_dataset[\"SalePrice\"].values, mdLineal.predict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(train_dataset[\"SalePrice\"].values, mdLineal.predict(),\n",
    "           scatter_kws={'alpha':0.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = np.exp(mdLineal.predict(test_dataset[most_relevant_features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'submission.csv'\n",
    "pd.DataFrame({'Id': test_dataset.Id, 'SalePrice': subm}).to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "np.sqrt(mean_squared_log_error(train_dataset[\"SalePrice\"].values, mdLineal.predict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "best_xgb_model = xgboost.XGBRegressor(colsample_bytree=0.4,\n",
    "                 gamma=0,                 \n",
    "                 learning_rate=0.01,\n",
    "                 max_depth=3,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=10000,                                                                    \n",
    "                 reg_alpha=0.9,\n",
    "                 reg_lambda=0.6,\n",
    "                 subsample=0.6,\n",
    "                 n_jobs=6)\n",
    "#params = {\n",
    "#        'learning_rate': [0.01, 0.03, 0.07],\n",
    "#        'max_depth': [3, 4],\n",
    "#        'min_child_weight': [1, 1.5, 2],\n",
    "#        'gamma': [0, 0.1, 0.2],\n",
    "#        'subsample': [0.6, 0.8, 1.0],\n",
    "#        'colsample_bytree': [0.2, 0.3, 0.4, 0.5],\n",
    "#        'max_depth': [3, 4, 5]\n",
    "#        }\n",
    "best_xgb_model.fit(train_dataset[most_relevant_features],train_dataset[\"SalePrice\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(train_dataset[\"SalePrice\"].values, best_xgb_model.predict(train_dataset[most_relevant_features]),\n",
    "           scatter_kws={'alpha':0.2})\n",
    "print(np.shape( best_xgb_model.predict(train_dataset[most_relevant_features])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_log_error(train_dataset[\"SalePrice\"].values, best_xgb_model.predict(train_dataset[most_relevant_features])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'submission.csv'\n",
    "subXGB = np.exp(best_xgb_model.predict(test_dataset[most_relevant_features]))\n",
    "pd.DataFrame({'Id': test_dataset.Id, 'SalePrice': subXGB}).to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subXGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdLasso = Lasso(0.00099, max_iter=50000)\n",
    "mdLasso.fit(train_dataset[most_relevant_features],train_dataset[\"SalePrice\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = mdLasso.predict(train_dataset[most_relevant_features])\n",
    "sns.regplot(y_hat, train_dataset[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_log_error(train_dataset[\"SalePrice\"].values, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.1\n",
    "fn_tr = beta * y_hat + (1-beta) * best_xgb_model.predict(train_dataset[most_relevant_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_log_error(train_dataset[\"SalePrice\"].values, fn_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_ts = beta * mdLasso.predict(test_dataset[most_relevant_features]) + (1-beta) * best_xgb_model.predict(test_dataset[most_relevant_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'submissionFN.csv'\n",
    "subFn = np.exp(fn_ts)\n",
    "pd.DataFrame({'Id': test_dataset.Id, 'SalePrice': subFn}).to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
