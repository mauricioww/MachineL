{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1. [Identify the Problem](#IP)\n",
    "2. [Import the Data](#ID)\n",
    "3. [Exploratory Data Analysis](#EDA)\n",
    "4. [Data Cleaning](#DC)\n",
    "     1. [Outlier Detection](#OD)\n",
    "     2. [Missing Value Imputation](#MVI)\n",
    "5. [Feature Transformation](#FT)\n",
    "     1. [Convert Categorical to Numerical Data](#C2N)\n",
    "     2. [Reduce Skewness](#RS)\n",
    "     3. [Normalise Data Columns](#NDC)\n",
    "6. [Modelling](#Mod)\n",
    "     1. [Model Learning](#ML)\n",
    "     2. [Model Validation](#MV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Identify the Problem <a id='IP'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, the problem statement says to predict the House Prices of given multiple variables.\n",
    "So. it is a Multivariable Regression Problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import the Data <a id='ID'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some useful info for the train set\n",
    "print(f'Train size is: {train.shape}')\n",
    "print(f'Test size is: {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis <a id='EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's Analyze the dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Skewness*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_plot_and_qqplot(data):\n",
    "    # Get the fitted parameters used by the function\n",
    "    (mu, sigma) = norm.fit(data)\n",
    "    print('mu = {:.2f} and sigma = {:.2f}'.format(mu, sigma))\n",
    "\n",
    "    # Plot the distribution\n",
    "    g = sns.distplot(data, fit=norm)\n",
    "    legend1 = plt.legend(['Skewness : {:.2f}'.format(data.skew())], loc=4)\n",
    "    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n",
    "    plt.gca().add_artist(legend1)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'{data.name} distribution')\n",
    "\n",
    "    # Get also the QQ-plot\n",
    "    fig = plt.figure()\n",
    "    res = stats.probplot(data, plot=plt)\n",
    "    plt.show()\n",
    "    \n",
    "distribution_plot_and_qqplot(train['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the target variable:\n",
    "* deviate from the normal distribution\n",
    "* have appreciable positive skewness\n",
    "* show peakedness\n",
    "   \n",
    "   \n",
    "As, the data in linear models should be normally distributed, later we will transform this variable and make it more normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Correlation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore the data, we will start with \n",
    "* Correlation matrix ie plotting heatmap.\n",
    "* Scatter plots between the most correlated variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize = (15,15))\n",
    "sns.heatmap(train.corr(), annot = True, linewidths=.5, fmt='.1f', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here, we can see few variables are highly correlated. Lets plot the 'SalePrice' correlation matrix (highly correlated variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corre = train.corr()\n",
    "top_corr_features = corre.index[abs(corre['SalePrice'])>0.5]\n",
    "g = sns.heatmap(train[top_corr_features].corr(),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the variables most correlated with 'SalePrice'. We can say:\n",
    "* 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'.\n",
    "* 'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables. However, we can say, the number of       cars that fit into the garage is a consequence of the garage area. 'GarageCars' and 'GarageArea' are like twin brothers.       You'll never be able to distinguish them. Therefore, we just need one of these variables in our analysis (we can keep          'GarageCars' since its correlation with 'SalePrice' is higher).\n",
    "* 'TotalBsmtSF' and '1stFlrSF' also seem to be twin brothers. We can keep 'TotalBsmtSF' just to say that our first guess was       right.\n",
    "* 'TotRmsAbvGrd' and 'GrLivArea', twin brothers again.\n",
    "* 'YearBuilt'... It seems that 'YearBuilt' is slightly correlated with 'SalePrice'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plots between 'SalePrice' and highly correlated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplot\n",
    "sns.set()\n",
    "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
    "sns.pairplot(train[cols], size = 2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we already know some of the main figures, this mega scatter plot gives us a reasonable idea about variables relationships.\n",
    "\n",
    "One of the figures we may find interesting is the one between 'TotalBsmtSF' and 'GrLiveArea'. In this figure we can see the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area.\n",
    "\n",
    "The plot concerning 'SalePrice' and 'YearBuilt' can also make us think. In the bottom of the 'dots cloud', we see what almost appears to be an exponential function. We can also see this same tendency in the upper limit of the 'dots cloud'. Also, notice how the set of dots regarding the last years tend to stay above this limit (I just wanted to say that prices are increasing faster now)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Cleaning <a id='DC'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Outlier Detection*<a id='OD'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before plotting let's create a useful function to use it again later\n",
    "def plot_scatter(x, y):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x=x, y=y)\n",
    "    plt.xlabel(x.name, fontsize=12)\n",
    "    plt.ylabel(y.name, fontsize=12)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(train['GrLivArea'], train['SalePrice'])\n",
    "plot_scatter(train['TotalBsmtSF'], train['SalePrice'])\n",
    "plot_scatter(train['1stFlrSF'], train['SalePrice'])\n",
    "plot_scatter(train['OverallQual'], train['SalePrice'])\n",
    "plot_scatter(train['GarageCars'], train['SalePrice'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are two point with (very) large value of GrLivArea and with (very) low price. Similarily, one point each  with (very) large value of TotalBsmtSF & 1stFlrSF and with (very) low price.\n",
    "These are outliers and we can safely remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "train[train.GrLivArea>4500]\n",
    "train[train.TotalBsmtSF>4000]\n",
    "train[train['1stFlrSF']>4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(train[train['Id']==524].index)\n",
    "train = train.drop(train[train['Id']==1299].index)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Missing Value Imputation*<a id='MVI'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine both train and test data to save our time and energy. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train,test])\n",
    "\n",
    "pd.set_option('display.max_rows',5000)\n",
    "pd.set_option('display.max_columns',500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the highly correlated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['GarageArea','1stFlrSF','TotRmsAbvGrd'], axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkig the columns for categorical and numerical values\n",
    "print(df.select_dtypes(include = ['int64','float64']).columns)\n",
    "print(df.select_dtypes(include = ['object']).columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing data\n",
    "total_miss = df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df.isnull().sum()/df.shape[0]*100).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total_miss,percent], axis=1, keys=['Total','Percent'])\n",
    "\n",
    "missing_data.head(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_drop =percent[percent > 20].keys()\n",
    "\n",
    "columns_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns_drop, axis = 1)\n",
    "\n",
    "print(df.shape)\n",
    "df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets impute the missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cols = df.columns[df.isnull().any()]\n",
    "\n",
    "missing_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First impute the missing values in Bsmt Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsmt_cols = ['BsmtCond', 'BsmtExposure', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtFinType1',\n",
    "       'BsmtFinType2', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtQual', 'BsmtUnfSF','TotalBsmtSF']\n",
    "\n",
    "bsmt_feat = df[bsmt_cols]\n",
    "bsmt_feat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first impute the missing values in rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsmt_feat = bsmt_feat[bsmt_feat.isnull().any(axis=1)]\n",
    "\n",
    "#print(bsmt_feat)\n",
    "print(bsmt_feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace the NaN in categorical with NA(ie No Basement) and with 0 in Numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsmt_feat_all_nan = bsmt_feat[(bsmt_feat.isnull() | bsmt_feat.isin([0])).all(1)]\n",
    "\n",
    "#print(bsmt_feat_all_nan)\n",
    "print(bsmt_feat_all_nan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qual = list(df.loc[:,df.dtypes=='object'].columns.values)\n",
    "\n",
    "for i in bsmt_cols:\n",
    "    if i in qual:\n",
    "        bsmt_feat_all_nan[i] = bsmt_feat_all_nan[i].replace(np.nan,'NA')\n",
    "    else:\n",
    "        bsmt_feat_all_nan[i] = bsmt_feat_all_nan[i].replace(np.nan,0)\n",
    "\n",
    "bsmt_feat.update(bsmt_feat_all_nan)\n",
    "df.update(bsmt_feat_all_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding remaining rows which have null columns\n",
    "\n",
    "bsmt_feat = bsmt_feat[bsmt_feat.isin([np.nan]).any(axis=1)]\n",
    "\n",
    "#print(bsmt_feat)\n",
    "print(bsmt_feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the BsmtFinType2 based on BsmtFinSF2 by bucketing the BsmtFinSF2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bucket the continuous columns\n",
    "print(df['BsmtFinSF2'].max())\n",
    "print(df['BsmtFinSF2'].min())\n",
    "\n",
    "#Bucket this  range in 5 buckets.\n",
    "#pd.cut(range(0,1526),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slice = df[(df['BsmtFinSF2'] >= 305) & (df['BsmtFinSF2'] <= 610)]\n",
    "\n",
    "#Impute this particular row\n",
    "bsmt_feat.at[333,'BsmtFinType2'] = df_slice['BsmtFinType2'].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the missing BsmtExposure value with the slice of BsmtExposure when BsmtQual is Gd.\n",
    "bsmt_feat['BsmtExposure'] = bsmt_feat['BsmtExposure'].replace(np.nan, df[df['BsmtQual'] == 'Gd']['BsmtExposure'].mode()[0])\n",
    "\n",
    "#Similarily\n",
    "bsmt_feat['BsmtCond'] = bsmt_feat['BsmtCond'].replace(np.nan, df['BsmtCond'].mode()[0])\n",
    "bsmt_feat['BsmtQual'] = bsmt_feat['BsmtQual'].replace(np.nan, df['BsmtQual'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.update(bsmt_feat)\n",
    "\n",
    "df.columns[df.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now impute the missing values in Garage Features.\n",
    "\n",
    "garage_cols = ['GarageCars', 'GarageCond', 'GarageFinish', 'GarageQual', 'GarageType',\n",
    "       'GarageYrBlt']\n",
    "\n",
    "gar_feat = df[garage_cols]\n",
    "gar_feat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gar_feat = gar_feat[gar_feat.isnull().any(axis=1)]\n",
    "\n",
    "#print(gar_feat)\n",
    "print(gar_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gar_feat_all_nan = gar_feat[(gar_feat.isnull() | gar_feat.isin([0])).all(1)]\n",
    "\n",
    "#print(gar_feat_all_nan)\n",
    "print(gar_feat_all_nan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in garage_cols:\n",
    "    if i in qual:\n",
    "        gar_feat_all_nan[i] = gar_feat_all_nan[i].replace(np.nan,'NA')\n",
    "    else:\n",
    "        gar_feat_all_nan[i] = gar_feat_all_nan[i].replace(np.nan,0)\n",
    "gar_feat.update(gar_feat_all_nan)\n",
    "df.update(gar_feat_all_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gar_feat = gar_feat[gar_feat.isnull().any(axis=1)]\n",
    "\n",
    "#gar_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in garage_cols:\n",
    "    gar_feat[i] = gar_feat[i].replace(np.nan, df[df['GarageType'] == 'Detchd'][i].mode()[0])\n",
    "\n",
    "#gar_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.update(gar_feat)\n",
    "\n",
    "df.columns[df.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode()[0])\n",
    "\n",
    "df['Exterior1st'] = df['Exterior1st'].fillna(df['Exterior1st'].mode()[0])\n",
    "\n",
    "df['Exterior2nd'] = df['Exterior2nd'].fillna(df['Exterior2nd'].mode()[0])\n",
    "\n",
    "df['Functional'] = df['Functional'].fillna(df['Functional'].mode()[0])\n",
    "\n",
    "df['KitchenQual'] = df['KitchenQual'].fillna(df['KitchenQual'].mode()[0])\n",
    "\n",
    "df['MSZoning'] = df['MSZoning'].fillna(df['MSZoning'].mode()[0])\n",
    "\n",
    "df['SaleType'] = df['SaleType'].fillna(df['SaleType'].mode()[0])\n",
    "\n",
    "df['Utilities'] = df['Utilities'].fillna(df['Utilities'].mode()[0])\n",
    "\n",
    "df['MasVnrType'] = df['MasVnrType'].fillna(df['MasVnrType'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['MasVnrArea'].isnull() == True]['MasVnrType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['MasVnrType'] == 'None') & (df['MasVnrArea'].isnull() == True), 'MasVnrArea'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "#print(df['MasVnrArea'].isnull().sum())\n",
    "#print(df['MasVnrType'].isnull().sum())\n",
    "#print(df.columns[df.isnull().any()])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now impute the LotFrontage based on LotConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotconfig = ['Corner','Inside','CulDSac','FR2','FR3']\n",
    "\n",
    "for i in lotconfig:\n",
    "    df['LotFrontage'] = pd.np.where((df['LotFrontage'].isnull() == True) & (df['LotConfig'] == i), df[df['LotConfig'] == i]['LotFrontage'].mean(),df['LotFrontage'])\n",
    "\n",
    "df.isnull().sum().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Feature Transformation <a id='FT'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dealing with the Categorical Data*<a id='C2N'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Few Features are in numerical in nature but actually are of Categorical\n",
    "\n",
    "cat_con_columns = ['MSSubClass', 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt','YrSold']\n",
    "for i in cat_con_columns:\n",
    "    df[i] = df[i].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "df['MoSold'] = df['MoSold'].apply(lambda x : calendar.month_abbr[x])\n",
    "\n",
    "df['MoSold'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quan = list(df.loc[:,df.dtypes != 'object'].columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# Ordered Data\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "df['BsmtCond'] = df['BsmtCond'].astype(CategoricalDtype(categories=['NA','Po','Fa','TA','Gd','Ex'], ordered = True)).cat.codes\n",
    "\n",
    "df['BsmtExposure'] = df['BsmtExposure'].astype(CategoricalDtype(categories=['NA','No','Mn','Av','Gd'], ordered = True)).cat.codes\n",
    "\n",
    "df['BsmtFinType1'] = df['BsmtFinType1'].astype(CategoricalDtype(categories=['NA','Unf','LwQ','Rec','BLQ','ALQ','GLQ'], ordered = True)).cat.codes\n",
    "\n",
    "df['BsmtFinType2'] = df['BsmtFinType2'].astype(CategoricalDtype(categories=['NA','Unf','LwQ','Rec','BLQ','ALQ','GLQ'], ordered = True)).cat.codes\n",
    "\n",
    "df['BsmtQual'] = df['BsmtQual'].astype(CategoricalDtype(categories=['NA','Po','Fa','TA','Gd','Ex'], ordered = True)).cat.codes\n",
    "\n",
    "df['ExterQual'] = df['BsmtQual'].astype(CategoricalDtype(categories=['Po','Fa','TA','Gd','Ex'], ordered = True)).cat.codes\n",
    "\n",
    "df['ExterCond'] = df['ExterCond'].astype(CategoricalDtype(categories=['Po','Fa','TA','Gd','Ex'], ordered = True)).cat.codes\n",
    "\n",
    "df['Functional'] = df['Functional'].astype(CategoricalDtype(categories=['Sal','Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'], ordered = True)).cat.codes\n",
    "\n",
    "df['GarageCond'] = df['GarageCond'].astype(CategoricalDtype(categories=['NA','Po','Fa','TA','Gd','Ex'], ordered = True)).cat.codes\n",
    "\n",
    "df['GarageQual'] = df['GarageQual'].astype(CategoricalDtype(categories=['NA','Po','Fa','TA','Gd','Ex'], ordered = True)).cat.codes\n",
    "\n",
    "df['GarageFinish'] = df['GarageFinish'].astype(CategoricalDtype(categories=['NA','Unf','RFn','Fin'], ordered = True)).cat.codes\n",
    "\n",
    "df['HeatingQC'] = df['HeatingQC'].astype(CategoricalDtype(categories=['Po','Fa','TA','Gd','Ex'], ordered = True)).cat.codes\n",
    "\n",
    "df['KitchenQual'] = df['KitchenQual'].astype(CategoricalDtype(categories=['Po','Fa','TA','Gd','Ex'], ordered = True)).cat.codes\n",
    "\n",
    "df['PavedDrive'] = df['PavedDrive'].astype(CategoricalDtype(categories=['N','P','Y'], ordered = True)).cat.codes\n",
    "\n",
    "df['Utilities'] = df['Utilities'].astype(CategoricalDtype(categories=['ELO','NoSeWa','NoSewr','AllPub'], ordered = True)).cat.codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reducing Skewness among numerical data* <a id='RS'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_features = ['2ndFlrSF','3SsnPorch',\n",
    " 'BedroomAbvGr','BsmtFinSF1','BsmtFinSF2',\n",
    " 'BsmtFullBath','BsmtHalfBath','BsmtUnfSF',\n",
    " 'EnclosedPorch','Fireplaces','FullBath',\n",
    " 'GarageCars','GrLivArea', 'HalfBath',\n",
    " 'KitchenAbvGr','LotArea','LotFrontage',\n",
    " 'LowQualFinSF','MasVnrArea','MiscVal',\n",
    " 'OpenPorchSF','PoolArea','ScreenPorch',\n",
    " 'TotalBsmtSF','WoodDeckSF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove Skewness from the data\n",
    "for i in skewed_features:\n",
    "    df[i] = np.log1p(df[i])\n",
    "\n",
    "log_SalePrice = np.log1p(train['SalePrice'])\n",
    "\n",
    "distribution_plot_and_qqplot(log_SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Normalisation* <a id='NDC'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dummies for all non ordinal categorical data\n",
    "qual1 = list(df.loc[:,df.dtypes == 'object'].columns.values)\n",
    "print(len(qual1))\n",
    "\n",
    "df_with_dummies = pd.get_dummies(df, columns=qual1, drop_first=True)\n",
    "df_with_dummies.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Normalize\n",
    "\n",
    "df_inputs = df_with_dummies.copy()\n",
    "targets = log_SalePrice.copy()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_inputs)\n",
    "df_inputs_scaled = scaler.transform(df_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Modelling <a id='Mod'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segregate data into original train and test\n",
    "train_len = len(train)\n",
    "train_scaled = df_inputs_scaled[:train_len]\n",
    "test_scaled = df_inputs_scaled[train_len:]\n",
    "\n",
    "print(train_scaled.shape)\n",
    "\n",
    "print(test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_scaled, targets, test_size=0.2, random_state=365)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Learning <a id='ML'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "regressor = xgboost.XGBRegressor(learning_rate = 0.06, max_depth= 3, n_estimators = 350, random_state= 0)\n",
    "regressor.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = regressor.predict(x_train)\n",
    "\n",
    "plt.scatter(y_train, y_hat, alpha = 0.2)\n",
    "plt.xlabel('Targets (y_train)',size=18)\n",
    "plt.ylabel('Predictions (y_hat)',size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Testing\n",
    "y_hat_test = regressor.predict(x_test)\n",
    "\n",
    "\n",
    "plt.scatter(y_test, y_hat_test, alpha=0.2)\n",
    "plt.xlabel('Targets (y_test)',size=18)\n",
    "plt.ylabel('Predictions (y_hat_test)',size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = regressor.predict(test_scaled)\n",
    "y_predict = np.expm1(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Validation <a id='MV'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "## k-Fold cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = regressor, X = x_train, y = y_train, cv = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracies.mean())\n",
    "print(accuracies.std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
